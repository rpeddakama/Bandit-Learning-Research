{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv7qosI3t3tO"
   },
   "source": [
    "# Goals and Pseudocode of Bandit Algorithms\n",
    "\n",
    "## Goals:\n",
    "input - arm chosen by the player (0 to k arms) <br>\n",
    "objective - maximize reward which is determined by minimizing regret <br>\n",
    "constraints - varies <br>\n",
    "model - varies <br>\n",
    "\n",
    "## Constants:\n",
    "k - # of arms on the bandit <br>\n",
    "numIterations - how many rounds the model is \"trained\" <br>\n",
    "\n",
    "## Methods:\n",
    "### Epsilon Greedy\n",
    "```\n",
    "initialize epsilon to some constant probability\n",
    "for t = 1...numIterations\n",
    "  prob = Randomly generate a probability from 0 to 1\n",
    "  if prob < epsilon:\n",
    "    pull a random arm\n",
    "  else:\n",
    "    pull the arm with the best experimental mean\n",
    "\n",
    "```\n",
    "### Thompson Sampling\n",
    "```\n",
    "start off with wide distributions for each arm\n",
    "for t = 1...numIterations\n",
    "    randomly choose a point on the beta distribution for each k-arm and pull the one with the largest calculated value\n",
    "```\n",
    "\n",
    "### Upper Confidence Bound\n",
    "```\n",
    "first k turns: initialize experimental means by pulling each arm once\n",
    "for t = 1...numIterations\n",
    "  for i = 1...k:\n",
    "    calculate a(i, t) = q_values[i] + sqrt((2*ln(t))/arm_counts[i])\n",
    "  pull arm i that maximizes experimental mean + a(i, t)\n",
    "\n",
    "```\n",
    "\n",
    "### UCB Pick and Compare (new)\n",
    "```\n",
    "first k turns: initialize experimental means by pulling each arm once\n",
    "for t = 1...numIterations\n",
    "  calculate a(random(k), t) = q_values[i] + sqrt((2*ln(t))/arm_counts[i])\n",
    "  pull arm i that is max of previous arm and randomly calculated arm (cost efficient)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "liN0yTcndgIA"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "dBi8TANl-tCg"
   },
   "outputs": [],
   "source": [
    "PLOT_HISTORY = True\n",
    "GATHER_DATA = True\n",
    "ALL_HISTORIES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "rGnka5mDeWgA"
   },
   "outputs": [],
   "source": [
    "# Function that graphs the average reward of an algorithm over time\n",
    "# Also graphs a barchart to show the frequency at which particular arms are chosen\n",
    "\n",
    "def plot_history(history):\n",
    "    rewards = history[\"rewards\"]\n",
    "    average_rewards = history[\"average_rewards\"]\n",
    "    chosen_arms = history[\"arms\"]\n",
    "    cumulative_rewards = history[\"cumulative_rewards\"]\n",
    "    cumulative_regrets = history[\"cumulative_regrets\"]\n",
    "\n",
    "    fig, axs = plt.subplots(4, figsize=[30, 30])\n",
    "    \n",
    "    axs[0].plot(cumulative_rewards, label = \"cumulative rewards\")\n",
    "    axs[0].set_title(\"Rewards\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"Rewards\")\n",
    "    \n",
    "    axs[1].plot(average_rewards, label = \"average rewards\")\n",
    "    axs[1].set_title(\"Average Rewards\")\n",
    "    axs[1].set_xlabel(\"Iteration\")\n",
    "    axs[1].set_ylabel(\"Reward\")\n",
    "    \n",
    "    axs[2].bar([i for i in range(len(chosen_arms))], chosen_arms, label = \"chosen arms\")\n",
    "    axs[2].set_title(\"Chosen Actions\")\n",
    "    axs[2].set_xlabel(\"Arm #\")\n",
    "    axs[2].set_ylabel(\"Number of times selected\") \n",
    "    \n",
    "    axs[3].plot(cumulative_regrets, label = \"cumulative regrets\")\n",
    "    axs[3].set_title(\"Regrets\")\n",
    "    axs[3].set_xlabel(\"Iteration\")\n",
    "    axs[3].set_ylabel(\"Regrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_cumulative(histories):\n",
    "    fig, axs = plt.subplots(4, 1, figsize=[10, 40])\n",
    "    LEGEND = ['Thompson Sampling', 'Epsilon Greedy', 'UCB', 'UCB Pick/Compare']\n",
    "\n",
    "    for i in range(len(histories)):\n",
    "        axs[0].plot(histories[i]['cumulative_rewards'], label = \"cumulative rewards\")\n",
    "        axs[0].set_title(\"Cumulative Rewards\")\n",
    "        axs[0].set_xlabel(\"Iteration\")\n",
    "        axs[0].set_ylabel(\"Rewards\")\n",
    "        axs[0].legend(LEGEND, loc='upper left')\n",
    "    \n",
    "        axs[1].plot(histories[i]['average_rewards'], label = \"average rewards\")\n",
    "        axs[1].set_title(\"Average Rewards\")\n",
    "        axs[1].set_xlabel(\"Iteration\")\n",
    "        axs[1].set_ylabel(\"Reward\")\n",
    "        axs[1].legend(LEGEND, loc='upper left')\n",
    "    \n",
    "        axs[2].bar([j for j in range(len(histories[i]['arms']))], histories[i]['arms'], label = \"chosen arms\")\n",
    "        axs[2].set_title(\"Chosen Actions\")\n",
    "        axs[2].set_xlabel(\"Arm #\")\n",
    "        axs[2].set_ylabel(\"Number of times selected\") \n",
    "        axs[2].legend(LEGEND, loc='upper left')\n",
    "    \n",
    "        axs[3].plot(histories[i]['cumulative_regrets'], label = \"cumulative regrets\")\n",
    "        axs[3].set_title(\"Regrets\")\n",
    "        axs[3].set_xlabel(\"Iteration\")\n",
    "        axs[3].set_ylabel(\"Regrets\")\n",
    "        axs[3].legend(LEGEND, loc='upper left')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "-3jWVH82eWkF"
   },
   "outputs": [],
   "source": [
    "# Initialize general environment to be used by all agents\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, rewards, deviations):\n",
    "        self.rewards = rewards\n",
    "        self.deviations = deviations\n",
    "        self.k = len(rewards)\n",
    "\n",
    "    def choose_arm(self, arm):\n",
    "        return np.random.normal(self.rewards[arm], self.deviations[arm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBDW8DDJZr43"
   },
   "source": [
    "# Random Agent\n",
    "This agent simply chooses random arms and pulls them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "YEodtVVEeWr0"
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "\n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "        cumulative_regrets = []\n",
    "\n",
    "        expected_value_sum = 0\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = np.random.choice(self.env.k)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_counts[arm] += 1\n",
    "            rewards.append(reward)\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            average_rewards.append(sum(rewards) / len(rewards))\n",
    "#             cumulative_regrets.append(i*np.amax(self.env.rewards) - sum(rewards))\n",
    "                       \n",
    "            expected_value_sum += self.env.rewards[arm]\n",
    "            cumulative_regrets.append(i*np.amax(self.env.rewards) - expected_value_sum)\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"cumulative_regrets\": cumulative_regrets, \"time\": execution_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bB8pDIhMrFxW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if GATHER_DATA:\n",
    "    random_agent = RandomAgent(env=environment, max_iterations=NUM_ITERATIONS)\n",
    "    RA_history = random_agent.act()\n",
    "    #ALL_HISTORIES.append(RA_history)\n",
    "    \n",
    "if PLOT_HISTORY:\n",
    "    print(\"RANDOM AGENT\")\n",
    "    print(f\"TOTAL REWARD : {sum(RA_history['rewards'])}\")\n",
    "    print(f\"TIME TAKEN (ms) : {RA_history['time']}\")\n",
    "\n",
    "    plot_history(RA_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eXNvi6X-gZL"
   },
   "source": [
    "# Thompson Sampling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "AK1uZxYk-gZM"
   },
   "outputs": [],
   "source": [
    "def draw_distributions(T, env, idx):\n",
    "    for i in range(env.k):\n",
    "        samps = np.random.normal(T.post_mu[i], T.post_sigma[i], 10000)\n",
    "        sns.kdeplot(samps, shade=True)\n",
    "    plt.title('Iteration %s'%(idx+1), fontsize=20)\n",
    "#     plt.legend(['mu=%s'%(env.rewards[i]) for i in range(env.k)], fontsize=16)\n",
    "    plt.xlim(-10,10)\n",
    "    plt.xlabel('Average Satisfacton', fontsize=20)\n",
    "    plt.ylabel('Density', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "hFhZPGms-gZM"
   },
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        self.prior_mu = np.zeros(self.env.k)\n",
    "        self.prior_sigma = np.full(self.env.k, 1000.0)\n",
    "        \n",
    "        self.post_mu = np.zeros(self.env.k)\n",
    "        self.post_sigma = np.full(self.env.k, 1000.0)\n",
    "        \n",
    "        self.n = np.zeros(env.k)\n",
    "        self.sum_satisfaction = np.zeros(env.k)\n",
    "        \n",
    "    def get_mu_from_current_distribution(self, arm):\n",
    "        return np.random.normal(self.post_mu[arm], self.post_sigma[arm])\n",
    "    \n",
    "    def update_current_distribution(self, arm):\n",
    "        self.post_sigma[arm] = np.sqrt((1 / self.prior_sigma[arm]**2 + self.n[arm])**-1)        \n",
    "        self.post_mu[arm] = (self.post_sigma[arm]**2) * (self.sum_satisfaction[arm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "erELBQf1-gZM"
   },
   "outputs": [],
   "source": [
    "def TS_act(environment, max_iterations=2000):\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    T = ThompsonSamplingAgent(environment)\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    cumulative_rewards = []\n",
    "    cumulative_regrets = []\n",
    "\n",
    "    expected_value_sum = 0\n",
    "    for i in range(1, max_iterations+1):\n",
    "    #     if environment.k <= 10 and (i < 10 or (i < 100 and (i+1) % 10 == 0) or ((i+1) % 100 == 0)):\n",
    "    #         draw_distributions(T, environment, i)\n",
    "\n",
    "        #get a sample from each posterior\n",
    "        post_samps = [T.get_mu_from_current_distribution(i) for i in range(environment.k)]\n",
    "\n",
    "        #index of distribution with highest satisfaction\n",
    "        chosen_idx = post_samps.index(max(post_samps))\n",
    "\n",
    "        #get a new sample from that distribution\n",
    "        s = T.env.choose_arm(chosen_idx)\n",
    "        T.n[chosen_idx] += 1\n",
    "        T.sum_satisfaction[chosen_idx] += s\n",
    "\n",
    "        #update other values\n",
    "        rewards.append(s)\n",
    "        average_rewards.append(sum(rewards)/len(rewards))\n",
    "        cumulative_rewards.append(sum(rewards))\n",
    "#         cumulative_regrets.append(i*np.amax(environment.rewards) - sum(rewards))\n",
    "\n",
    "        expected_value_sum += environment.rewards[chosen_idx]\n",
    "        cumulative_regrets.append(i*np.amax(environment.rewards) - expected_value_sum)\n",
    "    \n",
    "        #update that distributions posterior\n",
    "        T.update_current_distribution(chosen_idx)\n",
    "\n",
    "        \n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = (end_time - start_time)\n",
    "    execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "    return {\"arms\" : T.n, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "            \"cumulative_rewards\": cumulative_rewards, \"cumulative_regrets\": cumulative_regrets, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "WNAxMnXr-gZN"
   },
   "outputs": [],
   "source": [
    "if GATHER_DATA:\n",
    "    TS_history = TS_act(environment=environment, max_iterations=NUM_ITERATIONS)\n",
    "    ALL_HISTORIES.append(TS_history)\n",
    "\n",
    "if PLOT_HISTORY:\n",
    "    print(\"THOMPSON SAMPLING\")\n",
    "    print(f\"TOTAL REWARD : {sum(TS_history['rewards'])}\")\n",
    "    print(f\"TIME TAKEN (ms) : {TS_history['time']}\")    \n",
    "\n",
    "    plot_history(TS_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaoTH-DkZxIf"
   },
   "source": [
    "# Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "gE0PI8k3mRwt"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent():\n",
    "    def __init__(self, env, max_iterations=2000, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "        cumulative_regrets = []\n",
    "\n",
    "        for arm in range(self.env.k):\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "        \n",
    "        expected_value_sum = 0\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = np.random.randint(self.env.k) if np.random.random() < self.epsilon else np.argmax(q_values)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "#             cumulative_regrets.append(i*np.amax(self.env.rewards) - sum(rewards))\n",
    "\n",
    "            expected_value_sum += self.env.rewards[arm]\n",
    "            cumulative_regrets.append(i*np.amax(self.env.rewards) - expected_value_sum)\n",
    "                        \n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"cumulative_regrets\": cumulative_regrets, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "N2XOdN8Rq8P_"
   },
   "outputs": [],
   "source": [
    "if GATHER_DATA:\n",
    "    epsilon_greedy_agent = EpsilonGreedyAgent(env=environment, max_iterations=NUM_ITERATIONS, epsilon=0.1)\n",
    "    EG_history = epsilon_greedy_agent.act()\n",
    "    ALL_HISTORIES.append(EG_history)\n",
    "\n",
    "if PLOT_HISTORY:\n",
    "    print(\"EPSILON GREEDY\")\n",
    "    print(f\"TOTAL REWARD : {sum(EG_history['rewards'])}\")\n",
    "    print(f\"TIME TAKEN (ms) : {EG_history['time']}\")\n",
    "    \n",
    "    plot_history(EG_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1CMs-2AZ0kl"
   },
   "source": [
    "# Upper Confidence Bound (UCB) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "jkxZua7aS0ib"
   },
   "outputs": [],
   "source": [
    "class UpperConfidenceBoundAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "  \n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "        cumulative_regrets = []\n",
    "\n",
    "        #traverse every arm once\n",
    "        for arm in range(self.env.k):\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "        expected_value_sum = 0\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = 0\n",
    "            cur = 0\n",
    "            # finds action where quantity u + hoffdings-constant is maximized\n",
    "            for a in range(self.env.k):\n",
    "                UCB = q_values[a] + np.sqrt((2*np.log(i))/arm_counts[a])\n",
    "                if UCB > cur:\n",
    "                    cur = UCB\n",
    "                    arm = a\n",
    "\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "#             cumulative_regrets.append(i*np.amax(self.env.rewards) - sum(rewards))\n",
    "            \n",
    "            expected_value_sum += self.env.rewards[arm]\n",
    "            cumulative_regrets.append(i*np.amax(self.env.rewards) - expected_value_sum)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"cumulative_regrets\": cumulative_regrets, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "YJS6eKsxS0n2"
   },
   "outputs": [],
   "source": [
    "if GATHER_DATA:\n",
    "    UCB_agent = UpperConfidenceBoundAgent(env=environment, max_iterations=NUM_ITERATIONS)\n",
    "    UCB_history = UCB_agent.act()\n",
    "    ALL_HISTORIES.append(UCB_history)\n",
    "    \n",
    "if PLOT_HISTORY:\n",
    "    print(\"UCB AGENT\")\n",
    "    print(f\"TOTAL REWARD : {sum(UCB_history['rewards'])}\")\n",
    "    print(f\"TIME TAKEN (ms) : {UCB_history['time']}\")\n",
    "    \n",
    "    plot_history(UCB_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uys2f7izS0vl"
   },
   "source": [
    "# UCB Pick and Compare Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "_ennBrVH5LYl"
   },
   "outputs": [],
   "source": [
    "class UCB_PickAndCompareAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "  \n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "        cumulative_regrets = []\n",
    "\n",
    "        #traverse every arm once\n",
    "        for arm in range(self.env.k):\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "        # set default value for previous arm\n",
    "        previous_arm = 0\n",
    "        expected_value_sum = 0\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            UCB_previous = q_values[previous_arm] + np.sqrt((2*np.log(i))/arm_counts[previous_arm])\n",
    "            \n",
    "            compare_arm = np.random.choice(self.env.k)\n",
    "            UCB_compare = q_values[compare_arm] + np.sqrt((2*np.log(i))/arm_counts[compare_arm])\n",
    "\n",
    "            arm = (previous_arm if UCB_previous > UCB_compare else compare_arm)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            previous_arm = arm\n",
    "            \n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "#             cumulative_regrets.append(i*np.amax(self.env.rewards) - sum(rewards))\n",
    "            \n",
    "            expected_value_sum += self.env.rewards[arm]\n",
    "            cumulative_regrets.append(i*np.amax(self.env.rewards) - expected_value_sum)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"cumulative_regrets\": cumulative_regrets, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tEWrRp_25Ld4"
   },
   "outputs": [],
   "source": [
    "if GATHER_DATA:\n",
    "    UCB_PC_agent = UCB_PickAndCompareAgent(env=environment, max_iterations=NUM_ITERATIONS)\n",
    "    UCB_PC_history = UCB_PC_agent.act()\n",
    "    ALL_HISTORIES.append(UCB_PC_history)\n",
    "    \n",
    "if PLOT_HISTORY:\n",
    "    print(\"UCB_PC AGENT\")\n",
    "    print(f\"TOTAL REWARD : {sum(UCB_PC_history['rewards'])}\")\n",
    "    print(f\"TIME TAKEN (ms) : {UCB_PC_history['time']}\")\n",
    "    \n",
    "    plot_history(UCB_PC_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_HISTORY:\n",
    "    plot_history_cumulative(ALL_HISTORIES)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BanditLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
