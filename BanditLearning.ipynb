{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv7qosI3t3tO"
   },
   "source": [
    "# Goals and Pseudocode of Bandit Algorithms\n",
    "\n",
    "## Goals:\n",
    "input - arm chosen by the player (0 to k arms) <br>\n",
    "objective - maximize reward which is determined by minimizing regret <br>\n",
    "constraints - varies <br>\n",
    "model - varies <br>\n",
    "\n",
    "## Constants:\n",
    "k - # of arms on the bandit <br>\n",
    "numIterations - how many rounds the model is \"trained\" <br>\n",
    "\n",
    "## Methods:\n",
    "### Epsilon Greedy\n",
    "```\n",
    "initialize epsilon to some constant probability\n",
    "for t = 1...numIterations\n",
    "  prob = Randomly generate a probability from 0 to 1\n",
    "  if prob < epsilon:\n",
    "    pull a random arm\n",
    "  else:\n",
    "    pull the arm with the best experimental mean\n",
    "\n",
    "```\n",
    "### Thompson Sampling\n",
    "```\n",
    "start off with wide distributions for each arm\n",
    "for t = 1...numIterations\n",
    "    randomly choose a point on the beta distribution for each k-arm and pull the one with the largest calculated value\n",
    "```\n",
    "\n",
    "### Upper Confidence Bound\n",
    "```\n",
    "first k turns: initialize experimental means by pulling each arm once\n",
    "for t = 1...numIterations\n",
    "  for i = 1...k:\n",
    "    calculate a(i, t) = q_values[i] + sqrt((2*ln(t))/arm_counts[i])\n",
    "  pull arm i that maximizes experimental mean + a(i, t)\n",
    "\n",
    "```\n",
    "\n",
    "### UCB Pick and Compare (new)\n",
    "```\n",
    "first k turns: initialize experimental means by pulling each arm once\n",
    "for t = 1...numIterations\n",
    "  calculate a(random(k), t) = q_values[i] + sqrt((2*ln(t))/arm_counts[i])\n",
    "  pull arm i that is max of previous arm and randomly calculated arm (cost efficient)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "liN0yTcndgIA"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "rGnka5mDeWgA"
   },
   "outputs": [],
   "source": [
    "# Function that graphs the average reward of an algorithm over time\n",
    "# Also graphs a barchart to show the frequency at which particular arms are chosen\n",
    "\n",
    "def plot_history(history):\n",
    "    rewards = history[\"rewards\"]\n",
    "    average_rewards = history[\"average_rewards\"]\n",
    "    chosen_arms = history[\"arms\"]\n",
    "    cumulative_rewards = history[\"cumulative_rewards\"]\n",
    "\n",
    "    fig, axs = plt.subplots(3, figsize=[30, 30])\n",
    "    \n",
    "    axs[0].plot(cumulative_rewards, label = \"cumulative rewards\")\n",
    "    axs[0].set_title(\"Rewards\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"Rewards\")\n",
    "    \n",
    "    axs[1].plot(average_rewards, label = \"average rewards\")\n",
    "    axs[1].set_title(\"Average Rewards\")\n",
    "    axs[1].set_xlabel(\"Iteration\")\n",
    "    axs[1].set_ylabel(\"Reward\")\n",
    "    \n",
    "    axs[2].bar([i for i in range(len(chosen_arms))], chosen_arms, label = \"chosen arms\")\n",
    "    axs[2].set_title(\"Chosen Actions\")\n",
    "    axs[2].set_xlabel(\"Arm #\")\n",
    "    axs[2].set_ylabel(\"Number of times selected\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "-3jWVH82eWkF"
   },
   "outputs": [],
   "source": [
    "# Initialize general environment to be used by all agents\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, rewards, deviations):\n",
    "        self.rewards = rewards\n",
    "        self.deviations = deviations\n",
    "        self.k = len(rewards)\n",
    "\n",
    "    def choose_arm(self, arm):\n",
    "        return np.random.normal(self.rewards[arm], self.deviations[arm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "Tl5itzHzeWnO"
   },
   "outputs": [],
   "source": [
    "# nArms = 5\n",
    "# r = [1, 2, 3, 4, 5]\n",
    "# d = [1, 1, 1, 1, 0]\n",
    "# environment = Env(rewards=r, deviations=d)\n",
    "\n",
    "# environment = Env(rewards=np.random.randint(1, 50, nArms), deviations=np.random.randint(1, 10, nArms))\n",
    "\n",
    "# x = np.random.randint(1, 100, 20)\n",
    "# y = np.random.randint(1, 10, 20)\n",
    "# environment2 = Env(rewards=x, deviations=y)\n",
    "# for i in range(len(x)):\n",
    "#     print(x[i], y[i])\n",
    "#     print(environment2.choose_arm(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBDW8DDJZr43"
   },
   "source": [
    "# Random Agent\n",
    "This agent simply chooses random arms and pulls them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "YEodtVVEeWr0"
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "\n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = np.random.choice(self.env.k)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_counts[arm] += 1\n",
    "            rewards.append(reward)\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            average_rewards.append(sum(rewards) / len(rewards))\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNFiKUtcmRuQ",
    "outputId": "2d8033d0-715d-4fad-c0dd-1d808af2465b"
   },
   "outputs": [],
   "source": [
    "# random_agent = RandomAgent(env=environment, max_iterations=2000)\n",
    "# RA_history = random_agent.act()\n",
    "# print(f\"TOTAL REWARD : {sum(RA_history['rewards'])}\")\n",
    "# print(f\"TIME TAKEN (ms) : {RA_history['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "bB8pDIhMrFxW",
    "outputId": "2008430f-9053-4fac-d31f-d3e709341508",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_history(RA_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_distributions(T, env, idx):\n",
    "    for i in range(env.k):\n",
    "        samps = np.random.normal(T.post_mu[i], T.post_sigma[i], 10000)\n",
    "        sns.kdeplot(samps, shade=True)\n",
    "    plt.title('Iteration %s'%(idx+1), fontsize=20)\n",
    "#     plt.legend(['mu=%s'%(env.rewards[i]) for i in range(env.k)], fontsize=16)\n",
    "    plt.xlim(-10,10)\n",
    "    plt.xlabel('Average Satisfacton', fontsize=20)\n",
    "    plt.ylabel('Density', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        self.prior_mu = np.zeros(self.env.k)\n",
    "        self.prior_sigma = np.full(self.env.k, 1000.0)\n",
    "        \n",
    "        self.post_mu = np.zeros(self.env.k)\n",
    "        self.post_sigma = np.full(self.env.k, 1000.0)\n",
    "        \n",
    "        self.n = np.zeros(env.k)\n",
    "        self.sum_satisfaction = np.zeros(env.k)\n",
    "        \n",
    "    def get_mu_from_current_distribution(self, arm):\n",
    "        return np.random.normal(self.post_mu[arm], self.post_sigma[arm])\n",
    "    \n",
    "    def update_current_distribution(self, arm):\n",
    "        self.post_sigma[arm] = np.sqrt((1 / self.prior_sigma[arm]**2 + self.n[arm])**-1)        \n",
    "        self.post_mu[arm] = (self.post_sigma[arm]**2) * (self.sum_satisfaction[arm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_act(max_iterations):\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    T = ThompsonSamplingAgent(environment)\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "    #     if environment.k <= 10 and (i < 10 or (i < 100 and (i+1) % 10 == 0) or ((i+1) % 100 == 0)):\n",
    "    #         draw_distributions(T, environment, i)\n",
    "\n",
    "        #get a sample from each posterior\n",
    "        post_samps = [T.get_mu_from_current_distribution(i) for i in range(environment.k)]\n",
    "\n",
    "        #index of distribution with highest satisfaction\n",
    "        chosen_idx = post_samps.index(max(post_samps))\n",
    "\n",
    "        #get a new sample from that distribution\n",
    "        s = T.env.choose_arm(chosen_idx)\n",
    "        T.n[chosen_idx] += 1\n",
    "        T.sum_satisfaction[chosen_idx] += s\n",
    "\n",
    "        #update other values\n",
    "        rewards.append(s)\n",
    "        average_rewards.append(sum(rewards)/len(rewards))\n",
    "        cumulative_rewards.append(sum(rewards))\n",
    "\n",
    "        #update that distributions posterior\n",
    "        T.update_current_distribution(chosen_idx)\n",
    "\n",
    "        \n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = (end_time - start_time)\n",
    "    execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "    return {\"arms\" : T.n, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "                    \"cumulative_rewards\": cumulative_rewards, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS_history = TS_act()\n",
    "# print(f\"TOTAL REWARD : {sum(TS_history['rewards'])}\")\n",
    "# print(f\"TIME TAKEN (ms) : {TS_history['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(TS_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaoTH-DkZxIf"
   },
   "source": [
    "# Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "gE0PI8k3mRwt"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent():\n",
    "    def __init__(self, env, max_iterations=2000, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = np.random.randint(self.env.k) if np.random.random() < self.epsilon else np.argmax(q_values)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmT8WvnNmRzC",
    "outputId": "9505adb2-a403-4275-8f46-9483332cb9ec"
   },
   "outputs": [],
   "source": [
    "# epsilon_greedy_agent = EpsilonGreedyAgent(env=environment, max_iterations=2000, epsilon=0.1)\n",
    "# EG_history = epsilon_greedy_agent.act()\n",
    "# print(f\"TOTAL REWARD : {sum(EG_history['rewards'])}\")\n",
    "# print(f\"TIME TAKEN (ms) : {EG_history['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "N2XOdN8Rq8P_",
    "outputId": "00c85109-2a17-4fc2-b425-5a81b106a82e"
   },
   "outputs": [],
   "source": [
    "# plot_history(EG_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1CMs-2AZ0kl"
   },
   "source": [
    "# Upper Confidence Bound (UCB) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "jkxZua7aS0ib"
   },
   "outputs": [],
   "source": [
    "class UpperConfidenceBoundAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "  \n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        #traverse every arm once\n",
    "        for arm in range(self.env.k):\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = 0\n",
    "            cur = 0\n",
    "            # finds action where quantity u + hoffdings-constant is maximized\n",
    "            for a in range(self.env.k):\n",
    "                UCB = q_values[a] + np.sqrt((2*np.log(i))/arm_counts[a])\n",
    "                if UCB > cur:\n",
    "                    cur = UCB\n",
    "                    arm = a\n",
    "\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcnfKogDS0lK",
    "outputId": "15a16796-f02e-405f-cb79-764a26e8c52c"
   },
   "outputs": [],
   "source": [
    "# UCB_agent = UpperConfidenceBoundAgent(env=environment, max_iterations=2000)\n",
    "# UCB_history = UCB_agent.act()\n",
    "# print(f\"TOTAL REWARD : {sum(UCB_history['rewards'])}\")\n",
    "# print(f\"TIME TAKEN (ms) : {UCB_history['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "YJS6eKsxS0n2",
    "outputId": "f1a1c50e-c55b-4913-9d57-02eb1187ad63"
   },
   "outputs": [],
   "source": [
    "# plot_history(UCB_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uys2f7izS0vl"
   },
   "source": [
    "# UCB Pick and Compare Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "_ennBrVH5LYl"
   },
   "outputs": [],
   "source": [
    "class UCB_PickAndCompareAgent():\n",
    "    def __init__(self, env, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "  \n",
    "    def act(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        q_values = np.zeros(self.env.k)\n",
    "        arm_rewards = np.zeros(self.env.k)\n",
    "        arm_counts = np.zeros(self.env.k)\n",
    "\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        #traverse every arm once\n",
    "        for arm in range(self.env.k):\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "        # set default value for previous arm\n",
    "        previous_arm = 0\n",
    "        for i in range(1, self.iterations + 1):\n",
    "            UCB_previous = q_values[previous_arm] + np.sqrt((2*np.log(i))/arm_counts[previous_arm])\n",
    "            \n",
    "            compare_arm = np.random.choice(self.env.k)\n",
    "            UCB_compare = q_values[compare_arm] + np.sqrt((2*np.log(i))/arm_counts[compare_arm])\n",
    "\n",
    "            arm = (previous_arm if UCB_previous > UCB_compare else compare_arm)\n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            previous_arm = arm\n",
    "            \n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            cumulative_rewards.append(sum(rewards))\n",
    "            rewards.append(reward)\n",
    "            average_rewards.append(sum(rewards)/len(rewards))\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_diff = (end_time - start_time)\n",
    "        execution_time = time_diff.total_seconds()*1000\n",
    "                \n",
    "        return {\"arms\" : arm_counts, \"rewards\": rewards, \"average_rewards\": average_rewards,\n",
    "               \"cumulative_rewards\": cumulative_rewards, \"time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "nE_ekmU15LbR"
   },
   "outputs": [],
   "source": [
    "# UCB_PC_agent = UCB_PickAndCompareAgent(env=environment, max_iterations=2000)\n",
    "# UCB_PC_history = UCB_PC_agent.act()\n",
    "# print(f\"TOTAL REWARD : {sum(UCB_PC_history['rewards'])}\")\n",
    "# print(f\"TIME TAKEN (ms) : {UCB_PC_history['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "tEWrRp_25Ld4"
   },
   "outputs": [],
   "source": [
    "# plot_history(UCB_PC_history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BanditLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
